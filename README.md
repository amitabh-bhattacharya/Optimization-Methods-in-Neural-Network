## Optimization methods in Neural network

We will program to use advanced optimization methods in neural network that can speed up learning and perhaps even get us to a better final value for the cost function. Having a good optimization algorithm can be the difference between waiting days vs. just a few hours to get a good result.

The optimization techniques we will explore here are:
 - Mini-Batch Gradient descent.
 - Momentum (Exponentially weighted average).
 - Adam (one of the most effective optimization algorithms for training neural networks; Momentum + RMSprop).
 - Neural network model with the above optimization techniques on moons dataset.

